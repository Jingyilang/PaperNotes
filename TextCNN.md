## 主要方法
1.使用预训练的word2vec，但是要组成两个通道一为保持原有的结构，一为fine-tune（但是文中实验结果显示这种设计并不都好，我认为保持一个通道且可以fine-tune
应该就可以了，文中也显示了这种结构在有点数据集上的表现还优于双通道）
2.使用1-D conv在词矩阵上滑动 max-pooling后进入分类

## 理解
TextCNN 相似于传统自然语言处理中的n-gram语言模型，1-D conv 的filters的大小有[2, 3, 4](图示的2,3， 4，可是在文中3.1说的是3， 4， 5)分别就是
2-gram,3-gram,4-gram模型,然后模型集成共同作用于最后的分类，
